# --- Import Libraries ---
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster


# --- Load Dataset ---
df = pd.read_csv('sales_data_sample.csv', encoding='unicode_escape')
print("Dataset loaded successfully!")
print("Shape:", df.shape)
display(df.head())


df.info()


# --- Drop Unnecessary Columns ---
to_drop = ['ADDRESSLINE1', 'ADDRESSLINE2', 'STATE', 'POSTALCODE', 'PHONE']
df = df.drop(columns=to_drop, errors='ignore')


# --- Check for Nulls and Data Types ---
print("\nMissing Values:\n", df.isnull().sum())
print("\nData Types:\n", df.dtypes)


df.dtypes


# --- Select Numeric Columns ---
df_numeric = df.select_dtypes(include=['int64', 'float64']).copy()
print("\nNumeric Columns Selected:", df_numeric.columns.tolist())
# --- Handle Missing Values ---
df_numeric = df_numeric.replace([np.inf, -np.inf], np.nan)
df_numeric = df_numeric.fillna(df_numeric.mean())
# --- Detect Outliers Using IQR ---
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR
# --- Cap Outliers (instead of removing them) ---
df_capped = df_numeric.clip(lower=lower_bound, upper=upper_bound, axis=1)
print("Outliers capped successfully.")
# Select numeric columns only
df_numeric = df.select_dtypes(include=['int64', 'float64'])
# Visualize outliers
import matplotlib.pyplot as plt
import seaborn as sns
plt.figure(figsize=(12, 6))
sns.boxplot(data=df_numeric)
plt.title("Outlier Detection using Boxplots (Numeric Columns Only)")
plt.show()
# Identify outliers using IQR
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1
# Count outliers per column
outliers = ((df_numeric < (Q1 - 1.5 * IQR)) | (df_numeric > (Q3 + 1.5 * IQR))).sum()
print("\nNumber of Outliers per Numeric Feature:\n", outliers)


#normilization data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df_capped)
print("Data normalized using StandardScaler.")


df_normalized = pd.DataFrame(X_scaled, columns=df_capped.columns)
print("\nSample of Normalized Data:")
display(df_normalized.head())
print("\nMean of each feature after normalization:\n", df_normalized.mean())
print("\nStandard deviation of each feature after normalization:\n", df_normalized.std())


# --- Determine Optimal Clusters (Elbow Method) ---
inertia = []
K = range(1, 11)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(df_normalized)
    inertia.append(kmeans.inertia_)
plt.figure(figsize=(8, 5))
plt.plot(K, inertia, marker='o', color='orange')
plt.title("Elbow Method for Optimal k")
plt.xlabel("Number of Clusters (k)")
plt.ylabel("Inertia")
plt.grid(True)
plt.show()


from sklearn.metrics import silhouette_score
for k in range (2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(X_scaled)
    sil_score = silhouette_score(X_scaled, labels)
    print(f"K = {k} â†’ Silhouette Score = {sil_score:.4f}")


#Visualize Clusters for K = 3
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)
labels = kmeans.fit_predict(X_scaled)
plt.figure(figsize=(8,6))
sns.scatterplot(
    x=df_capped['SALES'],
    y=df_capped['MSRP'],
    hue=labels,
    palette='Set2'
)
plt.title("K-Means Clustering Visualization (K = 3)")
plt.xlabel("Sales")
plt.ylabel("MSRP")
plt.legend(title='Cluster')
plt.show()    


# --- Hierarchical Clustering (Optional) ---
linked = linkage(df_normalized, method='ward')
plt.figure(figsize=(10,5))
dendrogram(linked, truncate_mode='lastp', p=10)
plt.title("Hierarchical Clustering Dendrogram")
plt.xlabel("Cluster Size")
plt.ylabel("Distance")
plt.show()
hc_clusters = fcluster(linked, t=optimal_k, criterion='maxclust')
df['HC_Cluster'] = hc_clusters
print("Hierarchical Clustering also applied successfully.")
# --- Final Output ---
display(df[['Cluster', 'HC_Cluster']].head())
