import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
import seaborn as sns


# Load dataset
df = pd.read_csv("emails.csv")


df.head()


df.isnull().sum()


X = df.iloc[:,1:3001]  # word frequency features
X


Y = df.iloc[:,-1].values # 1 = spam, 0 = not spam
Y


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# Select only numeric columns
df_numeric = df.select_dtypes(include=['int64', 'float64'])
# Compute IQR for each numeric column
Q1 = df_numeric.quantile(0.25)
Q3 = df_numeric.quantile(0.75)
IQR = Q3 - Q1
# Define lower and upper bounds for outliers
lower = Q1 - 1.5 * IQR
upper = Q3 + 1.5 * IQR
# Create outlier mask
outlier_mask = (df_numeric < lower) | (df_numeric > upper)
# Count number of outliers per feature
outlier_counts = outlier_mask.sum().sort_values(ascending=False)
# Pick top N features with most outliers
topN = 12
top_features = outlier_counts.head(topN).index.tolist()
# Plot boxplots
plt.figure(figsize=(16, 6))
sns.boxplot(data=df_numeric[top_features])
plt.title(f"Boxplots for top {topN} features by outlier count")
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()


# Split data 
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42)


from sklearn.metrics import classification_report, confusion_matrix
# -------- Support Vector Machine --------
svc = SVC(C=1.0, kernel='rbf', gamma='auto')
svc.fit(X_train, y_train)
svc_pred = svc.predict(X_test)


print("SVM Accuracy:", accuracy_score(y_test, svc_pred))
print("SVM Classification Report:\n", classification_report(y_test, svc_pred))
print("SVM Confusion Matrix:\n", confusion_matrix(y_test, svc_pred))


# -------- K-Nearest Neighbors --------
knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train, y_train)
knn_pred = knn.predict(X_test)


print("KNN Accuracy:", knn.score(X_test, y_test))
print("KNN Classification Report:\n", classification_report(y_test, knn_pred))
print("KNN Confusion Matrix:\n", confusion_matrix(y_test, knn_pred))


# Import required libraries
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.model_selection import train_test_split
# Example: Split your data (replace 'df' and 'target_column' with your actual names)
# X = df.drop('target_column', axis=1)
# y = df['target_column']
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
# Standardize the features
scaler = StandardScaler()
X_train_s = scaler.fit_transform(X_train)
X_test_s = scaler.transform(X_test)
# List of k values to test
ks = [1, 3, 5]
# Dictionary to store results
results = {}
# Loop through different k values
for k in ks:
    knn = KNeighborsClassifier(n_neighbors=k, n_jobs=-1)
    knn.fit(X_train_s, y_train)
    y_pred = knn.predict(X_test_s)
    acc = accuracy_score(y_test, y_pred)
    cm = confusion_matrix(y_test, y_pred)
    report = classification_report(y_test, y_pred, zero_division=0)
    results[k] = acc
    print(f"\nK = {k}:")
    print(f"  Accuracy = {acc:.4f}")
    print("  Confusion Matrix:")
    print(cm)
    print("  Classification Report:")
    print(report)
# Optional: show best K value
best_k = max(results, key=results.get)
print(f"\n Best K = {best_k} with Accuracy = {results[best_k]:.4f}")
